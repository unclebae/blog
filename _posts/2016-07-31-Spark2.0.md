---
layout: post
title: Spark2.0 Release and has been changed
comments: true
---

## Spark 2.0

## 주요 업데이트 

2500개의 패치, 300 Contributor의 참여. 

    - API의 사용성이 증가. 
    - SQL2003지원
    - 성능의 개선
    - 구조화된 스트리밍
    - R UDF 지원
    - 내부처리 개선

### 1. API 안정성 향상. 
    - 2.X 버젼에서는 실험적인 버젼이 없이 모두 안정화된 버젼을 제공한다. 
    - 1.X 버젼과 대부분 동일하지만 모두 변경이 되었다. 

### 2. Core and Spark SQL

#### 2.1 Programming API
    - 통일된 DataFrame 과 Dataset : 
        - Scala, Java에서 DataFrame과 Dataset이 통일되었다. 
        - DataFrame는 단지 type alias이며 이는 Row의 데이터셋을 표현한다. 
        - 파이선과 R에서는 type safety가 부족했다. DataFrame은 메인 프로그래밍 인터페이스이다.
    - SparkSession:
        - 새로운 엔트리 포인트는 DataFrame과 Dataset API에 대한 오래된 SQLContext와 HiveContext 교체하였다.
        - SQLContext와 HiveContext는 backward compatibility를 유지한다.  
    - 신규, SparkSession을 위한 streamline 설정 API를 제공한다. 
    - 더 단순하고, 더 향상된 accumulator API제공. 
    - 신규, Dataset에서 타입 aggregation을 위한 향상된 Aggregator API 제공.  
    - 
#### 2.2 SQL
    - Spark 2.0에서는 SQL2003을 지원하며 더불어 매우 향상된 SQL기능을 제공한다. 
    - SparkSQL은 모든 99TPC-DS쿼리들을 수행할 수 있다. 
    - 향상된점 : 
        - Native SQL파서 : ANSI-SQL과 HiveQL 둘다 지원
        - Native DDL 커맨드 구현
        - 서브쿼리 지원
            - Uncorrelated Scalar Subqueries
            - Correlated Scalar Subqueries
            - NOT IN 조건 서브쿼리 (WHERE / HAVING 절에서 사용)
            - IN 조건 서브쿼리 (WHERE / HAVING 절에서 사용)
            - (NOT) EXISTS 조건 서브쿼리 (WHERE / HAVING 절에서 사용)
        - View 정규화 지원
    - Hive support 를 지원하는경우에 모든 기능은 대부분 Spark SQL에서 지원한다. 다만 Hive Connectivitiy와 Hive UDF, Script 변환은 에외이다.
    - 
#### 2.3 새로운 기능 
    - Native CSV 데이터 소스, 이는 Databricks의 [spark-csv module](https://github.com/databricks/spark-csv)를 기반으로한다. 
    - 캐시와 runtime exception 을 위한 Off-heap memory management 
    - Hive stype bucketing 지원
    - sketches를 이용한 거의 정확한 통계 서머리 지원, (quantile, Bloom filter, count-min sketch등 포함)
    - 
#### 2.4 성능, 런타임
    - SQL과 DataFrame처리에서 2 - 10배 빠른 퍼포먼스 제공, whole stage code generation 이라 불리는 기술을 이용하였다. 
    - vectorization을 이용한 향상된 Parquest scan 처리량.
    - 향상된 ORC성능
    - 일반적인 workloads를 위한 Catalyst query optimizer 의 많은 향상
    - 모든 윈도우 함수에 대한 네이티브 구현을 이용할때 윈도우 함수 퍼포먼스 향상
    - native data source를 위한 자동 파일 통합 

### 3. MLlib
    - DataFrame기반의 API는 메인 API가 되었다. RDD기반의 API는 관리 모드로 들어갔다. 
    - 
#### 3.1 새로운 기능 
        - ML persistence : DataFrames기반의 API는 ML모델의 저장과 로딩을 위한 거의 완벽한 기능을 제공한다. 또한 이는 Scala, Java, Python, R에서 파이프라인을 제공한다. 
        - R에서 MLlib : SparkR은 이제 generalized liner model, naive Bayes, k-means clustering, survival regression을 위한 MLlib API를 제공한다. 
        - Python : PySpark 은 많은 MLlib알고리즘을 제공한다. 이는 LDA, Gaussian Mixture Model, Generalized Linear Regression등 기타 다양한 모드를 지원한다. 
        - DataFrame 기반의 API가 추가된 알고리즘 : Bisecting K-means 클러스터링, Gaussian Mixture Model, MaxAbsScaler기능 변환 등을 제공. 
        - 
#### 3.2 Speed/Scaling
        - Vector과 Matric은 DataFrame에 저장되며 더 효과적으로 직렬화 하며, MLLib알고리즘을 호출할때 오버헤드를 더 줄여 주었다. 

### 4. SparkR
    - Spark 2.0에서 SparkR은 많은부분 개선이 있었다. 그중에서 user-deifned function (UDF)가 그렇다. 
    - 3가지 UDF가 있으며 dapply, gapply, lapply 가 그것이다. 
    - dapply와 gapply는 파티션 기반의 UDF를 사용할 수 있다. 
    - lapply는 hyper-parameter튜닝을 수행하는데 사용된다. 
    - 추가적인 기능 : 
        - R에서 머신러닝 알고리즘이 향상되었다. 이는 native Bayes, k-means clustering, survival regression과 같은 것이 추가되었다.
        - Generalized linear model은 더 많은 families와 link function을 제공한다. 
        - Save and load 가 모든 ML모델에 추가되었다. 
        - 더 많은 DataFrame 기능이 추가되었다. window 함수 api, reader, JDBC, CSV를 위한 reader추가, SparkSession 추가. 

### 5. Streaming
    - Spark 2.0은 structured streaming을 위한 실험적 기능을 릴리즈 했다. Spark SQL와 Catalyst optimizer의 최상위에 고수준의 스트리밍 API를 실었다. 
    - Structured Streaming은 사용자가 스트리밍 소스에 프로그림 할 수 있으며, 이는 동일한 DataFrame/Dataset API를 이용하여 static data source에 sink한다. 
    - Catalyst optimizer는 자동적으로 쿼리 플랜을 증가 시킨다. 
    - DStream API를 위한 주요한 업데이트는 새로운 실험으로 Kafka 0.10을 도입한 것이다. 

### 6. Dependency, Packaging, Operation
    - 스파크의 오퍼레이션과 패키징 처리과정에서는 많은 변화가 있다. 
        - Spark 2.0은 더이상 제품 버젼으로 fat assembly jar를 필요로 하지 않는다. 
        - Akka 의존성은 제거가 되었다. 그리고 결과적으로 사용자 어플리케이션은 은 특정 버젼의 Akka에 대해서 프로그램 될 수 있다. 
        - coarse grained Mesos mode 에서 복수개의 Mesos executor를 런칭할 수 있다. 
        - Kryo 버젼은 3.0이 되었다. 
        - 기본 빌드 버젼은 Scala 2.10에서 2.11로 되었다. 

### 7. Removals, Behavior Changes and Deprecations

#### 7.1 제거된것 
    - Bagel
    - 하둡 2.1이하버젼 지원 중단 
    - closure 직렬화 설정 기능 제거 
    - HTTPBroadcast
    - TTL-based metadata cleaning
    - Semi-private class org.apache.spark.Logging. We suggest you use slf4j directly.
    - SparkContext.metricsSystem
    - Block-oriented integration with Tachyon (subsumed by file system integration)
    - Methods deprecated in Spark 1.x
    - Methods on Python DataFrame that returned RDDs (map, flatMap, mapPartitions, etc). They are still available in dataframe.rdd field, e.g. dataframe.rdd.map.
    - Less frequently used streaming connectors, including Twitter, Akka, MQTT, ZeroMQ
    - Hash-based shuffle manager
    - History serving functionality from standalone Master
    - For Java and Scala, DataFrame no longer exists as a class. As a result, data sources would need to be updated.
    - Spark EC2 script has been fully moved to an external repository hosted by the UC Berkeley AMPLab

#### 7.2 Behavior Changes
    - 기본 빌드 버젼은 Spark 2.10에서 2.11으로 변경되었다. 
    - SQL에서 floating 리터럴들은 double data type 대신에 decimal data type으로 파싱된다. 
    - Kryo 버젼은 3.0이 되었다. 
    - Java RDD의 flatMap과 mapPartition함수는 Java Iterable 반환하게 변경되었다. 이로 인해서 모든 데이터를 구체화 할 필요가 없게 되었다. 
    - Java RDD의 countByKey와 countAprroxDistinctByKey 는 맵을 반환하며 K에 대해서 Long값이 반환된다. 예전에는 Object였다. 
    - Parquest 파일을 작성할때 서머리 파일은 기본적으로 작성되지 않는다. 이를 다시 기동시키기 위해서는 "parquest.enable.summary-metadata" 로 룰을 설정하면 된다. 
    - DataFrame기반의 API는 spark.mllib.linalg대신에 spark.ml.linalg 내에 있는 linear algebra 에 의존하도록 변경되었다. 이것은 spark.mllib.*.에서 spark.ml.* 의존성이 제거 되었다. 
     
#### 7.3 Deprecations
    - 다음 기능들은 deprecated되었다. 이후 버젼에서는 제거될 것이다. 
        - Fine-grained mode in Apache Mesos
        - Support For Java 7
        - Support for Python 2.6

       

{% if page.comments %}
<div id="disqus_thread"></div>
<script>
   /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document;
        s = d.createElement('script'); 
        s.src = '//https-unclebae-github-io.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
{% endif %}


